# Hyper-parameter informations
- dataset : adversary_cifar100 
- batch_size : 4 
- saved_model : ./models/VGG16/vgg16_adversary_cifar100_net.pth 

# Logs
Confusion matrix
[[39  4  0 ...  0  0  0]
 [ 0 36  0 ...  0  0  1]
 [ 1  4  4 ...  1  2  0]
 ...
 [ 0  1  0 ... 24  0  1]
 [ 0  1  1 ...  3 11  0]
 [ 0  0  0 ...  0  0 17]]

              precision    recall  f1-score   support

           0       0.65      0.39      0.49       100
           1       0.24      0.36      0.28       100
           2       0.18      0.04      0.07       100
           3       0.10      0.17      0.13       100
           4       0.09      0.08      0.09       100
           5       0.39      0.11      0.17       100
           6       0.14      0.15      0.14       100
           7       0.29      0.14      0.19       100
           8       0.16      0.23      0.19       100
           9       0.38      0.30      0.34       100
          10       0.10      0.07      0.08       100
          11       0.08      0.10      0.09       100
          12       0.26      0.22      0.24       100
          13       0.14      0.34      0.20       100
          14       0.12      0.15      0.13       100
          15       0.10      0.11      0.11       100
          16       0.33      0.31      0.32       100
          17       0.36      0.21      0.27       100
          18       0.17      0.21      0.19       100
          19       0.08      0.20      0.11       100
          20       0.66      0.53      0.59       100
          21       0.19      0.15      0.17       100
          22       0.30      0.19      0.23       100
          23       0.49      0.35      0.41       100
          24       0.39      0.58      0.47       100
          25       0.22      0.07      0.11       100
          26       0.15      0.24      0.18       100
          27       0.10      0.03      0.05       100
          28       0.45      0.25      0.32       100
          29       0.23      0.22      0.23       100
          30       0.21      0.14      0.17       100
          31       0.11      0.10      0.10       100
          32       0.12      0.21      0.15       100
          33       0.21      0.18      0.19       100
          34       0.13      0.23      0.17       100
          35       0.11      0.07      0.08       100
          36       0.32      0.22      0.26       100
          37       0.12      0.17      0.14       100
          38       0.11      0.15      0.13       100
          39       0.36      0.24      0.29       100
          40       0.20      0.24      0.22       100
          41       0.48      0.54      0.51       100
          42       0.15      0.10      0.12       100
          43       0.18      0.08      0.11       100
          44       0.07      0.10      0.08       100
          45       0.07      0.20      0.10       100
          46       0.10      0.15      0.12       100
          47       0.30      0.43      0.36       100
          48       0.21      0.45      0.28       100
          49       0.24      0.44      0.31       100
          50       0.18      0.04      0.07       100
          51       0.13      0.22      0.16       100
          52       0.43      0.36      0.39       100
          53       0.43      0.40      0.41       100
          54       0.22      0.35      0.27       100
          55       0.07      0.03      0.04       100
          56       0.19      0.34      0.24       100
          57       0.46      0.19      0.27       100
          58       0.23      0.14      0.17       100
          59       0.19      0.22      0.21       100
          60       0.60      0.36      0.45       100
          61       0.58      0.31      0.41       100
          62       0.34      0.43      0.38       100
          63       0.35      0.06      0.10       100
          64       0.19      0.11      0.14       100
          65       0.19      0.09      0.12       100
          66       0.13      0.17      0.15       100
          67       0.14      0.02      0.04       100
          68       0.41      0.43      0.42       100
          69       0.29      0.50      0.36       100
          70       0.28      0.22      0.24       100
          71       0.49      0.34      0.40       100
          72       0.09      0.08      0.08       100
          73       0.22      0.22      0.22       100
          74       0.07      0.06      0.07       100
          75       0.29      0.50      0.37       100
          76       0.50      0.33      0.40       100
          77       0.17      0.06      0.09       100
          78       0.07      0.04      0.05       100
          79       0.16      0.26      0.20       100
          80       0.11      0.03      0.05       100
          81       0.20      0.05      0.08       100
          82       0.41      0.47      0.44       100
          83       0.29      0.10      0.15       100
          84       0.15      0.14      0.14       100
          85       0.55      0.17      0.26       100
          86       0.44      0.41      0.42       100
          87       0.44      0.19      0.27       100
          88       0.10      0.13      0.11       100
          89       0.16      0.29      0.20       100
          90       0.13      0.30      0.18       100
          91       0.30      0.48      0.37       100
          92       0.40      0.12      0.18       100
          93       0.10      0.08      0.09       100
          94       0.67      0.39      0.49       100
          95       0.23      0.30      0.26       100
          96       0.15      0.08      0.11       100
          97       0.21      0.24      0.22       100
          98       0.21      0.11      0.14       100
          99       0.16      0.17      0.17       100

    accuracy                           0.22     10000
   macro avg       0.25      0.22      0.22     10000
weighted avg       0.25      0.22      0.22     10000
